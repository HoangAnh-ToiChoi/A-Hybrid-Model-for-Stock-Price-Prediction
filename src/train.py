import numpy as np
import os
import pandas as pd
from model import build_cnn_lstm_model
from config import TIME_STEP, EPOCH, BATCH_SIZE, LR, DATA_PATH
from preprocess import load_data # h√†m t·∫£i d·ªØ li·ªáu t·ª´ file csv
import glob 
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau


def train_model(csv_file):
    # L·∫•y d·ªØ li·ªáu t·ª´ file csv
    stock_symbol = os.path.basename(csv_file).replace('.csv', '')
    
    print(f"\n{'='*40}")
    print(f"ƒêang t·∫£i d·ªØ li·ªáu t·ª´ m√£ c·ªï phi·∫øu: {stock_symbol}\n")
    print(f"\n{'='*40}")

    try:
        x_train, y_train, x_test, y_test, scaler = load_data(csv_file, TIME_STEP)
        print(f"D·ªØ li·ªáu ƒë√£ {stock_symbol} ƒë∆∞·ª£c t·∫£i l√™n v√† ƒë√£ s·∫µn s√†ng üí™ \n")
        print(f"Shape train: {x_train.shape}\n")
        print(f"Shape test: {x_test.shape}\n")
    except Exception as e:
        print(f"L·ªói khi t·∫£i d·ªØ li·ªáu. L·ªói ƒë·ªçc file {csv_file}", e)
        return
    print("\n B·∫ÆT ƒê·∫¶U KH·ªûI T·∫†O MODEL HYBRID CNN-LSTM \n")
    model = build_cnn_lstm_model(time_step = TIME_STEP, features = 1, learning_rate = LR)
    model.summary()

    print (f"\n QU√Å TR√åNH TRAIN D·ªÆ LI·ªÜU {stock_symbol} B·∫ÆT D·∫¶U: ")

    checkpoint = [
            # L∆∞u model t·ªët nh·∫•t
    ModelCheckpoint(f"experiments/{stock_symbol}.keras", monitor='val_loss', save_best_only=True, verbose=1),
    
    # K·ª∏ THU·∫¨T M·ªöI: Gi·∫£m Learning Rate khi loss ƒëi ngang
    # N·∫øu val_loss kh√¥ng gi·∫£m sau 3 epoch -> chia ƒë√¥i t·ªëc ƒë·ªô h·ªçc
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),
    
    # D·ª´ng s·ªõm n·∫øu kh√¥ng kh√° h∆°n sau 10 epoch
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
    ]

    history = model.fit(
        x_train, y_train, # l·∫•y d·ªØ li·ªáu v√† k·∫øt qu·∫£ ra h·ªçc 
        validation_split = 0.2, # d√πng d·ªØ li·ªáu test v√† k·∫øt qu·∫£ test ƒë·ªÉ ki·ªÉm tra sau khi ƒë√£ h·ªçc h·∫øt 1000 c√¢u h·ªèi
        epochs = EPOCH, # l·∫∑p l·∫°i 100 l·∫ßn
        batch_size = BATCH_SIZE, # 1000 c√¢u h·ªèi th√¨ m·ªói l·∫ßn h·ªçc ch·ªâ 32 c√¢u ƒë·∫øn khi h·∫øt 1000 c√¢u th√¨ quay l·∫°i d√≤ng validation_data
        callbacks = checkpoint
    )

    save_dir = 'experiments'
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)
    save_path = os.path.join(save_dir, f"{stock_symbol}.keras")
    model.save(save_path)
    print(f"ƒë√£ l∆∞u model t·∫°i: {save_path}")


def main():
    csv_files = glob.glob(os.path.join(DATA_PATH, '*.csv'))

    if not csv_files:
        print(f"Kh√¥ng t√¨m th·∫•y file CSV trong th∆∞ m·ª•c {DATA_PATH}")
        return
    
    print(f"üîç T√¨m th·∫•y {len(csv_files)} file d·ªØ li·ªáu: {[os.path.basename(f) for f in csv_files]}")

    for csv_file in csv_files:
        train_model(csv_file)
    print("\nüéâ ƒê√£ ho√†n th√†nh training cho t·∫•t c·∫£ c√°c m√£ c·ªï phi·∫øu!")
    print("hehehe üöÄ")

if __name__ == '__main__':
    main()