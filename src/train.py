import numpy as np
import os
import pandas as pd
from model import build_cnn_lstm_model
from config import TIME_STEP, EPOCH, BATCH_SIZE, LR, DATA_PATH, MODELS_DIR, MODEL_PATH, SCALER_PATH
from preprocess import load_data # hÃ m táº£i dá»¯ liá»‡u tá»« file csv
import glob 
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import joblib

def train_model(csv_file):
    # Láº¥y dá»¯ liá»‡u tá»« file csv
    stock_symbol = os.path.basename(csv_file).replace('.csv', '')
    
    print(f"\n{'='*40}")
    print(f"Äang táº£i dá»¯ liá»‡u tá»« mÃ£ cá»• phiáº¿u: {stock_symbol}")
    print(f"{'='*40}")

    try:
        # Load dá»¯ liá»‡u (Äáº£m báº£o hÃ m load_data trong preprocess.py tráº£ vá» cáº£ scaler)
        x_train, y_train, x_test, y_test, scaler = load_data(csv_file, TIME_STEP)
        print(f"Dá»¯ liá»‡u {stock_symbol} Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn vÃ  Ä‘Ã£ sáºµn sÃ ng")
        print(f"   Shape train: {x_train.shape}")
        print(f"   Shape test: {x_test.shape}")
    except Exception as e:
        print(f"Lá»—i khi táº£i dá»¯ liá»‡u. Lá»—i Ä‘á»c file {csv_file}", e)
        return

    print("\nBáº®T Äáº¦U KHá»I Táº O MODEL HYBRID")
    model = build_cnn_lstm_model(time_step=TIME_STEP, features=1, learning_rate=LR)
    
    # --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN LÆ¯U ---
    model_save_path = os.path.join(MODELS_DIR, f"{stock_symbol}_best_model.keras")
    scaler_save_path = os.path.join(MODELS_DIR, f"{stock_symbol}_scaler.pkl")
    
    print(f"\nğŸƒ QUÃ TRÃŒNH TRAIN Dá»® LIá»†U {stock_symbol} Báº®T Dáº¦U: ")

    checkpoint = [
        # Sá»¬A Lá»–I 1: Bá» f"" Ä‘i, dÃ¹ng trá»±c tiáº¿p biáº¿n model_save_path
        ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, verbose=1),
        
        # Giáº£m tá»‘c Ä‘á»™ há»c náº¿u loss Ä‘i ngang
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),
        
        # Dá»«ng sá»›m náº¿u khÃ´ng khÃ¡ hÆ¡n sau 10 epoch
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
    ]

    history = model.fit(
        x_train, y_train, 
        # Sá»¬A Lá»–I 2: DÃ¹ng Ä‘Ãºng táº­p test Ä‘Ã£ chuáº©n bá»‹, khÃ´ng split thÃªm ná»¯a
        validation_data=(x_test, y_test), 
        epochs=EPOCH, 
        batch_size=BATCH_SIZE, 
        callbacks=checkpoint,
        verbose=1
    )

    # LÆ°u Scaler
    joblib.dump(scaler, scaler_save_path)
    print(f"LÆ°u Scaler táº¡i: {scaler_save_path}")
    print(f"Model tá»‘t nháº¥t Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng lÆ°u táº¡i: {model_save_path}")

def main():
    # Äáº£m báº£o thÆ° má»¥c models tá»“n táº¡i
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)

    csv_files = glob.glob(os.path.join(DATA_PATH, '*.csv'))

    if not csv_files:
        print(f"KhÃ´ng tÃ¬m tháº¥y file CSV trong thÆ° má»¥c {DATA_PATH}")
        return
    
    print(f"TÃ¬m tháº¥y {len(csv_files)} file dá»¯ liá»‡u: {[os.path.basename(f) for f in csv_files]}")

    for csv_file in csv_files:
        train_model(csv_file)
        
    print("\nğŸ‰ ÄÃ£ hoÃ n thÃ nh training cho táº¥t cáº£ cÃ¡c mÃ£ cá»• phiáº¿u!")
    print("hehehe ğŸš€")

if __name__ == '__main__':
    main()